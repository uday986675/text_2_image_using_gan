{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Text-to-Image GAN Demo (Low Compute 64\u00d764)\n", "A lightweight runnable notebook implementing a small-scale text-to-image GAN pipeline.\n", "This demo uses a synthetic shapes dataset (colored geometric shapes) to train a conditional GAN.\n", "You can run this on a single GPU or even CPU to see basic results."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cell 1: Imports\n", "import os, random, math\n", "from pathlib import Path\n", "import numpy as np\n", "from PIL import Image, ImageDraw\n", "import matplotlib.pyplot as plt\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "from torch.utils.data import Dataset, DataLoader\n", "from torchvision.utils import make_grid, save_image\n", "from tqdm import tqdm\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cell 2: Configuration\n", "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n", "OUT = Path('ttig_demo_outputs'); OUT.mkdir(exist_ok=True)\n", "IMG_SIZE, BATCH, Z_DIM, TEXT_DIM = 64, 64, 100, 32\n", "LR, EPOCHS, SEED = 2e-4, 30, 42\n", "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cell 3: Synthetic shapes dataset\n", "SHAPES = ['circle','square','triangle']\n", "COLORS = ['red','green','blue','yellow','magenta','cyan']\n", "\n", "def draw_shape(shape, color, size=64):\n", "    img = Image.new('RGB', (size,size), (255,255,255))\n", "    draw = ImageDraw.Draw(img)\n", "    pad = int(size*0.15)\n", "    bbox = [pad, pad, size-pad, size-pad]\n", "    color_map = {\n", "        'red':(230,25,75), 'green':(60,180,75), 'blue':(0,130,200),\n", "        'yellow':(255,225,25), 'magenta':(240,50,230), 'cyan':(70,240,240)\n", "    }\n", "    c = color_map[color]\n", "    if shape=='circle':\n", "        draw.ellipse(bbox, fill=c)\n", "    elif shape=='square':\n", "        draw.rectangle(bbox, fill=c)\n", "    elif shape=='triangle':\n", "        x0,y0,x1,y1 = bbox\n", "        pts = [(size/2,y0),(x1,y1),(x0,y1)]\n", "        draw.polygon(pts, fill=c)\n", "    return img\n", "\n", "class ShapesDataset(Dataset):\n", "    def __init__(self, n_images=2000, img_size=64):\n", "        self.records = []\n", "        for _ in range(n_images):\n", "            shape = random.choice(SHAPES)\n", "            color = random.choice(COLORS)\n", "            caption = f\"{color} {shape}\"\n", "            self.records.append({'shape':shape,'color':color,'caption':caption})\n", "        self.img_size = img_size\n", "\n", "    def __len__(self):\n", "        return len(self.records)\n", "\n", "    def __getitem__(self, idx):\n", "        rec = self.records[idx]\n", "        img = draw_shape(rec['shape'], rec['color'], self.img_size)\n", "        arr = torch.tensor(np.array(img).transpose(2,0,1)/127.5-1.0, dtype=torch.float32)\n", "        txt = rec['caption']\n", "        return arr, txt\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cell 4: Text embedding + models\n", "class SimpleTextEmbed(nn.Module):\n", "    def __init__(self, vocab):\n", "        super().__init__()\n", "        self.word_to_idx = {w:i for i,w in enumerate(vocab)}\n", "        self.emb = nn.Embedding(len(vocab), TEXT_DIM)\n", "    def forward(self, captions):\n", "        ids = []\n", "        for cap in captions:\n", "            toks = cap.split()\n", "            ids.append([self.word_to_idx[t] for t in toks])\n", "        return self.emb(torch.tensor(ids)) .mean(1)\n", "\n", "class Generator(nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.fc = nn.Sequential(nn.Linear(Z_DIM+TEXT_DIM, 512*4*4), nn.ReLU(True))\n", "        self.net = nn.Sequential(\n", "            nn.ConvTranspose2d(512,256,4,2,1), nn.BatchNorm2d(256), nn.ReLU(True),\n", "            nn.ConvTranspose2d(256,128,4,2,1), nn.BatchNorm2d(128), nn.ReLU(True),\n", "            nn.ConvTranspose2d(128,64,4,2,1), nn.BatchNorm2d(64), nn.ReLU(True),\n", "            nn.Conv2d(64,3,3,1,1), nn.Tanh())\n", "    def forward(self, z, txt):\n", "        x = torch.cat([z,txt],1)\n", "        x = self.fc(x).view(-1,512,4,4)\n", "        return self.net(x)\n", "\n", "class Discriminator(nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.conv = nn.Sequential(\n", "            nn.Conv2d(3,64,4,2,1), nn.LeakyReLU(0.2,True),\n", "            nn.Conv2d(64,128,4,2,1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2,True),\n", "            nn.Conv2d(128,256,4,2,1), nn.BatchNorm2d(256), nn.LeakyReLU(0.2,True),\n", "            nn.Conv2d(256,512,4,2,1), nn.BatchNorm2d(512), nn.LeakyReLU(0.2,True))\n", "        self.fc_img = nn.Linear(512*4*4, 1)\n", "        self.fc_txt = nn.Linear(TEXT_DIM, 512*4*4)\n", "    def forward(self, img, txt):\n", "        h = self.conv(img).view(img.size(0), -1)\n", "        proj = torch.sum(h * self.fc_txt(txt), 1, keepdim=True)\n", "        return self.fc_img(h) + proj\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cell 5: Training loop\n", "dataset = ShapesDataset(1000, IMG_SIZE)\n", "vocab = sorted(list(set(sum([c.split() for _,c in dataset],[]))))\n", "text_embed = SimpleTextEmbed(vocab).to(DEVICE)\n", "G, D = Generator().to(DEVICE), Discriminator().to(DEVICE)\n", "optG = torch.optim.Adam(G.parameters(), lr=LR, betas=(0.5,0.999))\n", "optD = torch.optim.Adam(D.parameters(), lr=LR, betas=(0.5,0.999))\n", "loader = DataLoader(dataset, batch_size=BATCH, shuffle=True)\n", "\n", "def d_loss(real_pred, fake_pred):\n", "    return torch.mean(F.relu(1.0 - real_pred)) + torch.mean(F.relu(1.0 + fake_pred))\n", "\n", "def g_loss(fake_pred):\n", "    return -torch.mean(fake_pred)\n", "\n", "for epoch in range(EPOCHS):\n", "    for imgs, caps in tqdm(loader):\n", "        imgs = imgs.to(DEVICE)\n", "        txt = text_embed(caps).to(DEVICE)\n", "        z = torch.randn(imgs.size(0), Z_DIM, device=DEVICE)\n", "        fake = G(z, txt)\n", "\n", "        real_pred = D(imgs, txt)\n", "        fake_pred = D(fake.detach(), txt)\n", "        lossD = d_loss(real_pred, fake_pred)\n", "        optD.zero_grad(); lossD.backward(); optD.step()\n", "\n", "        fake_pred_g = D(fake, txt)\n", "        lossG = g_loss(fake_pred_g)\n", "        optG.zero_grad(); lossG.backward(); optG.step()\n", "\n", "    print(f\"Epoch {epoch+1}/{EPOCHS}  LossD={lossD.item():.3f}  LossG={lossG.item():.3f}\")\n", "    with torch.no_grad():\n", "        sample_z = torch.randn(16, Z_DIM, device=DEVICE)\n", "        sample_txt = text_embed([random.choice(COLORS)+' '+random.choice(SHAPES) for _ in range(16)]).to(DEVICE)\n", "        samples = G(sample_z, sample_txt)\n", "        save_image((samples+1)/2, OUT/f'sample_{epoch:03d}.png', nrow=4)\n", "print('Training complete! Check ttig_demo_outputs for images.')\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 5}