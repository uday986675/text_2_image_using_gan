{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0876b26d",
   "metadata": {},
   "source": [
    "# Text-to-Image GAN Demo (Low Compute 64×64)\n",
    "A lightweight runnable notebook implementing a small-scale text-to-image GAN pipeline.\n",
    "This demo uses a synthetic shapes dataset (colored geometric shapes) to train a conditional GAN.\n",
    "You can run this on a single GPU or even CPU to see basic results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f037bd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import os, random, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b44959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "OUT = Path('ttig_demo_outputs'); OUT.mkdir(exist_ok=True)\n",
    "IMG_SIZE, BATCH, Z_DIM, TEXT_DIM = 64, 64, 100, 32\n",
    "LR, EPOCHS, SEED = 2e-4, 30, 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edaa833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Synthetic shapes dataset\n",
    "SHAPES = ['circle','square','triangle']\n",
    "COLORS = ['red','green','blue','yellow','magenta','cyan']\n",
    "\n",
    "def draw_shape(shape, color, size=64):\n",
    "    img = Image.new('RGB', (size,size), (255,255,255))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    pad = int(size*0.15)\n",
    "    bbox = [pad, pad, size-pad, size-pad]\n",
    "    color_map = {\n",
    "        'red':(230,25,75), 'green':(60,180,75), 'blue':(0,130,200),\n",
    "        'yellow':(255,225,25), 'magenta':(240,50,230), 'cyan':(70,240,240)\n",
    "    }\n",
    "    c = color_map[color]\n",
    "    if shape=='circle':\n",
    "        draw.ellipse(bbox, fill=c)\n",
    "    elif shape=='square':\n",
    "        draw.rectangle(bbox, fill=c)\n",
    "    elif shape=='triangle':\n",
    "        x0,y0,x1,y1 = bbox\n",
    "        pts = [(size/2,y0),(x1,y1),(x0,y1)]\n",
    "        draw.polygon(pts, fill=c)\n",
    "    return img\n",
    "\n",
    "class ShapesDataset(Dataset):\n",
    "    def __init__(self, n_images=2000, img_size=64):\n",
    "        self.records = []\n",
    "        for _ in range(n_images):\n",
    "            shape = random.choice(SHAPES)\n",
    "            color = random.choice(COLORS)\n",
    "            caption = f\"{color} {shape}\"\n",
    "            self.records.append({'shape':shape,'color':color,'caption':caption})\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.records[idx]\n",
    "        img = draw_shape(rec['shape'], rec['color'], self.img_size)\n",
    "        arr = torch.tensor(np.array(img).transpose(2,0,1)/127.5-1.0, dtype=torch.float32)\n",
    "        txt = rec['caption']\n",
    "        return arr, txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Text embedding + models\n",
    "class SimpleTextEmbed(nn.Module):\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__()\n",
    "        self.word_to_idx = {w:i for i,w in enumerate(vocab)}\n",
    "        self.emb = nn.Embedding(len(vocab), TEXT_DIM)\n",
    "    def forward(self, captions):\n",
    "        ids = []\n",
    "        for cap in captions:\n",
    "            toks = cap.split()\n",
    "            ids.append([self.word_to_idx[t] for t in toks])\n",
    "        return self.emb(torch.tensor(ids)) .mean(1)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(Z_DIM+TEXT_DIM, 512*4*4), nn.ReLU(True))\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512,256,4,2,1), nn.BatchNorm2d(256), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256,128,4,2,1), nn.BatchNorm2d(128), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128,64,4,2,1), nn.BatchNorm2d(64), nn.ReLU(True),\n",
    "            nn.Conv2d(64,3,3,1,1), nn.Tanh())\n",
    "    def forward(self, z, txt):\n",
    "        x = torch.cat([z,txt],1)\n",
    "        x = self.fc(x).view(-1,512,4,4)\n",
    "        return self.net(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3,64,4,2,1), nn.LeakyReLU(0.2,True),\n",
    "            nn.Conv2d(64,128,4,2,1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2,True),\n",
    "            nn.Conv2d(128,256,4,2,1), nn.BatchNorm2d(256), nn.LeakyReLU(0.2,True),\n",
    "            nn.Conv2d(256,512,4,2,1), nn.BatchNorm2d(512), nn.LeakyReLU(0.2,True))\n",
    "        self.fc_img = nn.Linear(512*4*4, 1)\n",
    "        self.fc_txt = nn.Linear(TEXT_DIM, 512*4*4)\n",
    "    def forward(self, img, txt):\n",
    "        h = self.conv(img).view(img.size(0), -1)\n",
    "        proj = torch.sum(h * self.fc_txt(txt), 1, keepdim=True)\n",
    "        return self.fc_img(h) + proj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc7f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Training loop\n",
    "dataset = ShapesDataset(1000, IMG_SIZE)\n",
    "vocab = sorted(list(set(sum([c.split() for _,c in dataset],[]))))\n",
    "text_embed = SimpleTextEmbed(vocab).to(DEVICE)\n",
    "G, D = Generator().to(DEVICE), Discriminator().to(DEVICE)\n",
    "optG = torch.optim.Adam(G.parameters(), lr=LR, betas=(0.5,0.999))\n",
    "optD = torch.optim.Adam(D.parameters(), lr=LR, betas=(0.5,0.999))\n",
    "loader = DataLoader(dataset, batch_size=BATCH, shuffle=True)\n",
    "\n",
    "def d_loss(real_pred, fake_pred):\n",
    "    return torch.mean(F.relu(1.0 - real_pred)) + torch.mean(F.relu(1.0 + fake_pred))\n",
    "\n",
    "def g_loss(fake_pred):\n",
    "    return -torch.mean(fake_pred)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for imgs, caps in tqdm(loader):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        txt = text_embed(caps).to(DEVICE)\n",
    "        z = torch.randn(imgs.size(0), Z_DIM, device=DEVICE)\n",
    "        fake = G(z, txt)\n",
    "\n",
    "        real_pred = D(imgs, txt)\n",
    "        fake_pred = D(fake.detach(), txt)\n",
    "        lossD = d_loss(real_pred, fake_pred)\n",
    "        optD.zero_grad(); lossD.backward(); optD.step()\n",
    "\n",
    "        fake_pred_g = D(fake, txt)\n",
    "        lossG = g_loss(fake_pred_g)\n",
    "        optG.zero_grad(); lossG.backward(); optG.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}  LossD={lossD.item():.3f}  LossG={lossG.item():.3f}\")\n",
    "    with torch.no_grad():\n",
    "        sample_z = torch.randn(16, Z_DIM, device=DEVICE)\n",
    "        sample_txt = text_embed([random.choice(COLORS)+' '+random.choice(SHAPES) for _ in range(16)]).to(DEVICE)\n",
    "        samples = G(sample_z, sample_txt)\n",
    "        save_image((samples+1)/2, OUT/f'sample_{epoch:03d}.png', nrow=4)\n",
    "print('Training complete! Check ttig_demo_outputs for images.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0fce72",
   "metadata": {},
   "source": [
    "# Improvements: Text preprocessing, BERT embeddings, and Evaluation\n",
    "This appended section improves your notebook technically:\n",
    "\n",
    "1. **Text preprocessing**: normalization, punctuation removal, optional stopword removal, and preparing data for BERT tokenization.\n",
    "\n",
    "2. **BERT embeddings**: example code using Hugging Face `transformers` to get sentence / token embeddings (pooled output or average of token vectors).\n",
    "\n",
    "3. **Evaluation metrics**: functions to compute Inception Score (IS) and Frechet Inception Distance (FID) using torchvision's InceptionV3 activations. These are implemented as reusable functions — you can run them on folders of images (real vs generated) or on tensors.\n",
    "\n",
    "4. **Explanations and inline comments**: each code block includes comments and rationale so you can understand and modify as required.\n",
    "\n",
    "> Notes: running the cells requires internet access the first time to download pretrained models (BERT / Inception) and may require installing packages. An installation cell is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830fb9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install necessary packages. Run this cell once.\n",
    "# If you're running on Google Colab, add an exclamation mark (!) before pip, e.g. !pip install ...\n",
    "import sys\n",
    "import subprocess\n",
    "def pip_install(pkg):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "# Recommended packages\n",
    "pkgs = [\n",
    "    \"transformers>=4.0.0\",     # Hugging Face transformers for BERT embeddings\n",
    "    \"torch\",                   # PyTorch (may already be installed in your environment)\n",
    "    \"torchvision\",             # For Inception model and image utilities\n",
    "    \"tqdm\",\n",
    "    \"numpy\",\n",
    "    \"scipy\",\n",
    "    \"pillow\",\n",
    "    \"nltk\"\n",
    "]\n",
    "for p in pkgs:\n",
    "    try:\n",
    "        __import__(p.split('==')[0])\n",
    "    except Exception:\n",
    "        print(\"Installing\", p)\n",
    "        pip_install(p)\n",
    "\n",
    "# Download NLTK data for preprocessing\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "print('Setup done — restart the kernel if necessary.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa63823",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Text preprocessing and BERT tokenization example.\n",
    "# This cell demonstrates:\n",
    "# - Normalization (lowercasing, unicode normalization)\n",
    "# - Punctuation removal\n",
    "# - Optional stopword removal using NLTK\n",
    "# - Conversion to token ids / attention masks via Hugging Face tokenizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import re\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Unicode normalize, lowercase, strip\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "def clean_text(text, remove_stopwords=False):\n",
    "    text = normalize_text(text)\n",
    "    # Remove punctuation (keep basic tokens); you can customize regex\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # Reduce multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    if remove_stopwords:\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "        text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "examples = [\n",
    "    \"A small red bird sitting on a branch, looking to the left.\",\n",
    "    \"An astronaut riding a horse on Mars!\"\n",
    "]\n",
    "cleaned = [clean_text(t, remove_stopwords=True) for t in examples]\n",
    "print('Original -> Cleaned:')\n",
    "for o,c in zip(examples, cleaned):\n",
    "    print('-', o, '->', c)\n",
    "\n",
    "# Tokenize with BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "enc = tokenizer(cleaned, padding=True, truncation=True, max_length=64, return_tensors='pt')\n",
    "print('\\nTokenized keys:', enc.keys())\n",
    "print('input_ids shape:', enc['input_ids'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599b8275",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Obtain BERT embeddings (two common strategies):\n",
    "# 1) Use pooled_output (CLS token) as a sentence embedding.\n",
    "# 2) Average token embeddings (excluding padding) for a mean-pooled embedding.\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "def get_bert_embeddings(input_ids, attention_mask, pooling='cls'):\n",
    "    '''\n",
    "    input_ids, attention_mask: torch tensors (batch, seq_len)\n",
    "    pooling: 'cls' or 'mean'\n",
    "    Returns: (batch, hidden_size) tensor\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden = out.last_hidden_state  # (B, L, H)\n",
    "        pooled = out.pooler_output           # (B, H) — corresponds to [CLS] after dense+tanh\n",
    "        if pooling == 'cls':\n",
    "            return pooled\n",
    "        elif pooling == 'mean':\n",
    "            # Compute mean over valid tokens only\n",
    "            mask = attention_mask.unsqueeze(-1).expand_as(last_hidden).float()\n",
    "            summed = (last_hidden * mask).sum(1)\n",
    "            counts = mask.sum(1).clamp(min=1e-9)\n",
    "            return summed / counts\n",
    "        else:\n",
    "            raise ValueError('Unknown pooling type')\n",
    "\n",
    "# Example: compute embeddings for cleaned examples\n",
    "embs = get_bert_embeddings(enc['input_ids'], enc['attention_mask'], pooling='mean')\n",
    "print('BERT embeddings shape (mean-pooled):', embs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9245190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Functions to compute Inception activations, Frechet Inception Distance (FID), and Inception Score (IS).\n",
    "# Implementation notes:\n",
    "# - We use torchvision's pretrained InceptionV3 (transform input to 299x299 and apply required preprocessing)\n",
    "# - FID computes statistics (mean, covariance) of activations and compares real vs generated sets.\n",
    "# - IS computes KL divergence between conditional label distribution and marginal label distribution using softmax outputs from Inception.\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Preprocessing to match InceptionV3 expected inputs\n",
    "_inception_transform = transforms.Compose([\n",
    "    transforms.Resize((299,299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "def load_image_tensor(path):\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    return _inception_transform(img).unsqueeze(0)  # (1,3,299,299)\n",
    "\n",
    "# Load inception model\n",
    "_inception = models.inception_v3(pretrained=True, transform_input=False).to(_device)\n",
    "_inception.eval()\n",
    "\n",
    "# We will use the pool3 features (2048-d) for FID. For torchvision's inception, we can access 'Mixed_7c' output via forward hook.\n",
    "# Simpler approach: run inception and take the last pooling layer 'fc' input. We'll use the model's forward to obtain features\n",
    "from torch import nn\n",
    "class InceptionActivations(nn.Module):\n",
    "    def __init__(self, inception_model):\n",
    "        super().__init__()\n",
    "        # copy model up to the last pooling layer\n",
    "        self.model = inception_model\n",
    "    def forward(self, x):\n",
    "        # Inception forward returns logits; to get features we run through model until the pool layer\n",
    "        # Use the model's forward but intercept features; torchvision's inception has attribute 'AuxLogits' and uses different branches.\n",
    "        # A pragmatic (but slightly slower) approach: run model, then grab the penultimate layer before fc by forward hooks.\n",
    "        # For clarity and reproducibility we will run the model and extract the 'Mixed_7c' activations via hooks.\n",
    "        raise RuntimeError('Use helper function get_activations_from_files for ease.')\n",
    "\n",
    "def get_activations_from_files(file_list, batch_size=8):\n",
    "    # Compute activations (2048-d pool3) for a list of image file paths\n",
    "    acts = []\n",
    "    _inception.to(_device)\n",
    "    for i in range(0, len(file_list), batch_size):\n",
    "        batch_paths = file_list[i:i+batch_size]\n",
    "        batch = torch.cat([load_image_tensor(p) for p in batch_paths], dim=0).to(_device)\n",
    "        # InceptionV3 in torchvision expects a special forward when aux_logits present; set transform_input False.\n",
    "        with torch.no_grad():\n",
    "            # Run model up to last pooling\n",
    "            preds = _inception(batch)\n",
    "            # Unfortunately torchvision's inception returns logits; to get pool features we can access the model's last bottleneck.\n",
    "            # As a workaround, run the model and compute features using adaptive avg pool on the final convolution output.\n",
    "            # Accessing internal layers:\n",
    "            # The following navigates the model to get final conv features — this depends on torchvision implementation.\n",
    "            try:\n",
    "                # try accessing 'Mixed_7c' output by running forward through children\n",
    "                x = batch\n",
    "                for name, module in _inception.named_children():\n",
    "                    if name == 'fc' or name == 'AuxLogits':\n",
    "                        break\n",
    "                    x = module(x)\n",
    "                # x should now be of shape (B, 2048, 1, 1)\n",
    "                feat = nn.functional.adaptive_avg_pool2d(x, (1,1)).reshape(x.size(0), -1)\n",
    "            except Exception as e:\n",
    "                # Fallback: use logits as features (not ideal but still usable for coarse comparisons)\n",
    "                feat = preds\n",
    "            acts.append(feat.cpu().numpy())\n",
    "    acts = np.concatenate(acts, axis=0)\n",
    "    return acts\n",
    "\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    # From original FID implementation\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "    diff = mu1 - mu2\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "    # Numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2*covmean)\n",
    "    return fid\n",
    "\n",
    "def compute_fid(real_files, gen_files, batch_size=8):\n",
    "    act1 = get_activations_from_files(real_files, batch_size=batch_size)\n",
    "    act2 = get_activations_from_files(gen_files, batch_size=batch_size)\n",
    "    mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n",
    "    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n",
    "    fid_value = calculate_frechet_distance(mu1, sigma1, mu2, sigma2)\n",
    "    return float(fid_value)\n",
    "\n",
    "def inception_score_from_probs(preds, eps=1e-16):\n",
    "    # preds: numpy array (N, num_classes) of softmax scores from Inception logits\n",
    "    p_y = np.mean(preds, axis=0)\n",
    "    kl = preds * (np.log(preds + eps) - np.log(p_y + eps))\n",
    "    sum_kl = np.sum(kl, axis=1)\n",
    "    return float(np.exp(np.mean(sum_kl)))\n",
    "\n",
    "def compute_inception_score_from_files(file_list, splits=10, batch_size=8):\n",
    "    # Compute softmax predictions for each image using inception, then compute IS\n",
    "    preds = []\n",
    "    for i in range(0, len(file_list), batch_size):\n",
    "        batch_paths = file_list[i:i+batch_size]\n",
    "        batch = torch.cat([load_image_tensor(p) for p in batch_paths], dim=0).to(_device)\n",
    "        with torch.no_grad():\n",
    "            logits = _inception(batch)\n",
    "            soft = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()\n",
    "            preds.append(soft)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    # Compute IS by splitting\n",
    "    N = preds.shape[0]\n",
    "    split_scores = []\n",
    "    split_size = N // splits\n",
    "    for k in range(splits):\n",
    "        part = preds[k*split_size: (k+1)*split_size]\n",
    "        split_scores.append(inception_score_from_probs(part))\n",
    "    return float(np.mean(split_scores)), float(np.std(split_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ba780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Usage examples for evaluation:\n",
    "# 1) Save generated images (PIL) to a folder 'generated/' and have a folder 'real/' with real images.\n",
    "# 2) Run compute_fid(real_files, gen_files) and compute_inception_score_from_files(gen_files).\n",
    "import glob\n",
    "# Example paths (replace with your actual folders)\n",
    "real_folder = 'real_images/'\n",
    "gen_folder = 'generated_images/'\n",
    "real_files = sorted(glob.glob(real_folder + '*.png')) + sorted(glob.glob(real_folder + '*.jpg'))\n",
    "gen_files = sorted(glob.glob(gen_folder + '*.png')) + sorted(glob.glob(gen_folder + '*.jpg'))\n",
    "\n",
    "if len(real_files) == 0 or len(gen_files) == 0:\n",
    "    print('No images found in example folders. Save generated images to', gen_folder, 'and real images to', real_folder)\n",
    "else:\n",
    "    fid_val = compute_fid(real_files, gen_files, batch_size=8)\n",
    "    print('FID:', fid_val)\n",
    "    is_mean, is_std = compute_inception_score_from_files(gen_files, splits=10, batch_size=8)\n",
    "    print('Inception Score (mean, std):', is_mean, is_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae0345",
   "metadata": {},
   "source": [
    "\n",
    "# Recommended next steps after these improvements:\n",
    "# - Replace dummy / small generator with your full text-to-image GAN architecture.\n",
    "# - Use BERT token embeddings: you may want to fine-tune BERT or use static embeddings depending on dataset size.\n",
    "# - For cross-attention: pass token-level embeddings (B, T, H) into your CrossAttention block so each token can attend to spatial features.\n",
    "# - Compute FID between a held-out validation set of real images and generated images (>= 1000 images gives more stable FID, but you can start with 100-500).\n",
    "# - Consider additional metrics: CLIP-based similarity (semantic similarity between prompt and image), precision/recall for generative models, and diversity metrics.\n",
    "# - Add more visualizations: image grids, t-SNE of embeddings, and attention maps to inspect what tokens attend to what regions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
